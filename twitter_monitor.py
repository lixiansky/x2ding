import os
import time
import random
import json
import requests
from datetime import datetime
from playwright.sync_api import sync_playwright
from playwright_stealth import stealth_sync
from bs4 import BeautifulSoup

# é…ç½®
USERS_STR = os.environ.get('TWITTER_USER', 'elonmusk')
USERS = [u.strip() for u in USERS_STR.split(',') if u.strip()]
WEBHOOK_URL = os.environ.get('DINGTALK_WEBHOOK')

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LAST_ID_FILE = os.path.join(BASE_DIR, 'last_id.json')

# å¤‡é€‰ Nitter å®ä¾‹ (ä»…ä½œä¸ºåŸŸåå‚è€ƒ)
NITTER_INSTANCES = [
    'https://xcancel.com',
    'https://nitter.privacyredirect.com',
    'https://nitter.poast.org',
    'https://nitter.hu',
    'https://nitter.moomoo.me',
    'https://nitter.net',
]

INSTANCES_FILE = os.path.join(BASE_DIR, 'instances.json')

def get_random_user_agent():
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0"
    ]
    return random.choice(ua_list)

def load_instances():
    """
    ä»æœ¬åœ°ç¼“å­˜åŠ è½½å¥åº·çš„ Nitter å®ä¾‹
    """
    if os.path.exists(INSTANCES_FILE):
        try:
            with open(INSTANCES_FILE, 'r', encoding='utf-8') as f:
                instances = json.load(f)
                if instances and isinstance(instances, list):
                    print(f"[ç³»ç»Ÿ] æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½ {len(instances)} ä¸ªå®ä¾‹")
                    return instances
        except Exception as e:
            print(f"[ç³»ç»Ÿ] åŠ è½½å®ä¾‹ç¼“å­˜å¤±è´¥: {e}")
    
    print("[ç³»ç»Ÿ] ç¼“å­˜ä¸å­˜åœ¨æˆ–æŸåï¼Œé‡‡ç”¨å†…ç½®å…œåº•å®ä¾‹åˆ—è¡¨")
    return NITTER_INSTANCES

def scrape_nitter_with_playwright(target, dynamic_instances=None):
    """
    ä½¿ç”¨ Playwright æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—® Nitter å¹¶æŠ“å–æœ€æ–°æ¨æ–‡
    """
    is_search = target.startswith('search:')
    keyword = target[7:] if is_search else target
    
    # ä¼˜å…ˆä½¿ç”¨åŠ¨æ€è·å–çš„å®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨å†…ç½®çš„
    instances = dynamic_instances if dynamic_instances else NITTER_INSTANCES.copy()
    # ä¸ºäº†åˆ†å¸ƒå‹åŠ›ï¼Œæˆ‘ä»¬åœ¨ä¿æŒé«˜åˆ†å®ä¾‹åœ¨å‰çš„å‰æä¸‹ï¼Œå¯¹å‰ 5 åè¿›è¡Œå°èŒƒå›´éšæœº
    if len(instances) > 5:
        top_5 = instances[:5]
        random.shuffle(top_5)
        others = instances[5:]
        random.shuffle(others)
        instances = top_5 + others
    else:
        random.shuffle(instances)
    
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ (å¤´æ¨¡å¼/æ— å¤´æ¨¡å¼å–å†³äºç¯å¢ƒï¼ŒGitHub Actions å»ºè®® headless=True)
        browser = p.chromium.launch(headless=True)
        
        for instance in instances:
            try:
                # æ¯ä¸ªå®ä¾‹åˆ›å»ºä¸€ä¸ªæ–°ä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹Ÿå¹²å‡€çš„è®¿é—®
                context = browser.new_context(
                    user_agent=get_random_user_agent(),
                    viewport={'width': 1280, 'height': 720}
                )
                page = context.new_page()
                
                # åº”ç”¨ Stealth æ’ä»¶ç»•è¿‡æ£€æµ‹
                stealth_sync(page)
                
                if is_search:
                    url = f"{instance.rstrip('/')}/search?f=tweets&q={requests.utils.quote(keyword)}"
                else:
                    url = f"{instance.rstrip('/')}/{keyword}"
                
                print(f"[{target}] æ­£åœ¨åŠ è½½: {url}")
                
                # å¼€å§‹åŠ è½½å¹¶å¤„ç†å¯èƒ½çš„æŒ‘æˆ˜
                response = page.goto(url, wait_until="networkidle", timeout=45000)
                
                # å¦‚æœçœ‹åˆ° "Verifying your browser"ï¼Œç­‰å¾…å…¶æ¶ˆå¤±
                if "Verifying your browser" in page.content():
                    print(f"[{target}] æ£€æµ‹åˆ°æµè§ˆå™¨éªŒè¯ï¼Œå°è¯•ç­‰å¾…...")
                    # æŸäº›éªŒè¯éœ€è¦ä¸€ç‚¹æ—¶é—´è‡ªåŠ¨è·³è½¬
                    page.wait_for_timeout(5000)
                
                # è·å–æœ€ç»ˆæ¸²æŸ“åçš„ HTML
                html = page.content()
                soup = BeautifulSoup(html, 'html.parser')
                
                # Nitter é¡µé¢æ¨æ–‡è§£æé€»è¾‘
                items = soup.select('.timeline-item')
                if not items:
                    print(f"[{target}] åœ¨å®ä¾‹ {instance} ä¸Šæœªå‘ç°æ¨æ–‡å†…å®¹")
                    context.close()
                    continue
                
                # æ‰«æç­–ç•¥ï¼šæ‰«æå‰ 5 æ¡æ¨æ–‡ï¼Œæ‰¾åˆ°ç¬¬ä¸€æ¡éç½®é¡¶çš„ã€æœ‰æ•ˆçš„å†…å®¹
                valid_tweets = []
                for item in items[:8]: # æ‰©å¤§æ‰«æèŒƒå›´åˆ°å‰ 8 æ¡
                    # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ç½®é¡¶æ¨æ–‡
                    is_pinned = item.select_one('.pinned') or "Pinned" in item.get_text()
                    if is_pinned:
                        print(f"[{target}] å‘ç°ç½®é¡¶æ¨æ–‡ï¼Œè·³è¿‡æ‰«æ")
                        continue
                    
                    # 2. æ£€æŸ¥æ˜¯å¦æ˜¯è½¬å‘
                    is_retweet = item.select_one('.retweet-header') is not None

                    # 3. æå–å›¾ç‰‡
                    images = []
                    # Nitter çš„å›¾ç‰‡é€šå¸¸åœ¨ .attachment.image æˆ– .tweet-image ä¸­
                    img_els = item.select('.attachment.image img, .tweet-image img')
                    for img in img_els:
                        src = img.get('src', '')
                        if src:
                            # è½¬æ¢ç›¸å¯¹è·¯å¾„
                            full_src = instance.rstrip('/') + src if src.startswith('/') else src
                            images.append(full_src)

                    # æå–å…³é”®ä¿¡æ¯
                    content_el = item.select_one('.tweet-content')
                    link_el = item.select_one('.tweet-link')
                    date_el = item.select_one('.tweet-date a')
                    author_el = item.select_one('.username')

                    if not content_el or not link_el:
                        continue

                    # æå–æ¨æ–‡ ID (ä» /user/status/123...#m ä¸­æå–æ•°å­—)
                    link_href = link_el.get('href', '')
                    tweet_id = link_href.split('/status/')[-1].split('#')[0] if '/status/' in link_href else link_href

                    tweet_data = {
                        'content': content_el.get_text(strip=True),
                        'link': instance.rstrip('/') + link_href,
                        'published': date_el.get('title', '') if date_el else 'Unknown Time',
                        'author': author_el.get_text(strip=True) if author_el else keyword,
                        'guid': tweet_id,
                        'is_retweet': is_retweet,
                        'images': images
                    }
                    valid_tweets.append(tweet_data)
                    
                    # åªè¦æ‰¾åˆ°äº†ç¬¬ä¸€ä¸ªéç½®é¡¶çš„æœ‰æ•ˆæ¨æ–‡ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºå®ƒæ˜¯å½“å‰â€œæœ€æ–°çš„â€
                    if len(valid_tweets) >= 1:
                        break

                if valid_tweets:
                    tweet = valid_tweets[0]
                    retweet_tag = " [è½¬å‘]" if tweet['is_retweet'] else ""
                    print(f"[{target}] æˆåŠŸä» {instance} æŠ“å–{retweet_tag}æ¨æ–‡: {tweet['guid']}")
                    context.close()
                    browser.close()
                    return tweet

                print(f"[{target}] {instance} é¡µé¢ä¸Šæœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„éç½®é¡¶æ¨æ–‡")
                context.close()

            except Exception as e:
                print(f"[{target}] è®¿é—® {instance} å‡ºé”™: {e}")
                continue
        
        browser.close()
    return None

def send_dingtalk(webhook_url, tweet, target):
    """
    å‘é€é’‰é’‰æ¶ˆæ¯
    """
    if not webhook_url:
        print("æœªé…ç½® DINGTALK_WEBHOOKï¼Œè·³è¿‡å‘é€")
        return False

    retweet_flag = " ğŸ”ƒ è½¬å‘äº†" if tweet.get('is_retweet') else " ğŸ“ å‘å¸ƒäº†"
    
    # æ„é€ å›¾ç‰‡ Markdown (ä½¿ç”¨ weserv.nl ä»£ç†è§£å†³å›½å†…é’‰é’‰åŠ è½½ä¸å‡ºçš„é—®é¢˜)
    images_md = ""
    if tweet.get('images'):
        for img_url in tweet['images']:
            # ç¼–ç  URL å¹¶åŒ…è£…ä»£ç†
            proxied_url = f"https://images.weserv.nl/?url={requests.utils.quote(img_url.replace('https://', '').replace('http://', ''))}"
            images_md += f"\n\n![image]({proxied_url})"

    title = f"Twitter ç›‘æ§: {target}"
    text = f"""## {target}{retweet_flag} æ¨æ–‡
---
**ä½œè€…**: {tweet['author']}
**æ—¶é—´**: {tweet['published']}

> {tweet['content']}
{images_md}

---
[ğŸ”— Nitter åŸæ–‡]({tweet['link']}) | [ğŸ”— Twitter(X) åŸæ–‡]({tweet['link'].replace('xcancel.com', 'twitter.com').replace('nitter.net', 'twitter.com').replace('nitter.hu', 'twitter.com').replace('nitter.privacyredirect.com', 'twitter.com').replace('nitter.poast.org', 'twitter.com')})
    """

    data = {
        "msgtype": "markdown",
        "markdown": {
            "title": title,
            "text": text
        }
    }

    try:
        resp = requests.post(webhook_url, json=data, timeout=10)
        result = resp.json()
        if result.get('errcode') == 0:
            print(f"[{target}] é’‰é’‰æ¨é€æˆåŠŸ")
            return True
        else:
            print(f"[{target}] é’‰é’‰æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        print(f"[{target}] é’‰é’‰è¯·æ±‚å¼‚å¸¸: {e}")
        return False

def main():
    if not USERS:
        print("æ²¡æœ‰é…ç½®ç›‘æ§ç›®æ ‡")
        return

    print(f"[{datetime.now()}] å¯åŠ¨ Playwright åæ£€æµ‹ç›‘æ§æ¨¡å¼...")
    
    # ä»æœ¬åœ°ç¼“å­˜åŠ è½½å¯ç”¨å®ä¾‹
    instances = load_instances()

    # åŠ è½½çŠ¶æ€
    if os.path.exists(LAST_ID_FILE):
        try:
            with open(LAST_ID_FILE, 'r', encoding='utf-8') as f:
                last_ids = json.load(f)
        except: last_ids = {}
    else: last_ids = {}

    updated = False
    for target in USERS:
        try:
            tweet = scrape_nitter_with_playwright(target, instances)
            if not tweet:
                continue
            
            current_id = tweet['guid']
            if last_ids.get(target) != current_id:
                print(f"[{target}] å‘ç°æ›´æ–°: {current_id}")
                if send_dingtalk(WEBHOOK_URL, tweet, target):
                    last_ids[target] = current_id
                    updated = True
            else:
                print(f"[{target}] æ— è§†æ›´æ–°")
        except Exception as e:
            print(f"[{target}] æ€»ä½“å¤„ç†å¼‚å¸¸: {e}")

    if updated:
        with open(LAST_ID_FILE, 'w', encoding='utf-8') as f:
            json.dump(last_ids, f, indent=2, ensure_ascii=False)
        print("çŠ¶æ€æ–‡ä»¶å·²æ›´æ–°")

if __name__ == "__main__":
    main()
