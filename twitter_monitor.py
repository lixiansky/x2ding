import os
import time
import random
import json
import requests
from datetime import datetime
from playwright.sync_api import sync_playwright
from playwright_stealth import stealth_sync
from bs4 import BeautifulSoup
import tempfile
import base64

# é…ç½®
USERS_STR = os.environ.get('TWITTER_USER', 'elonmusk')
USERS = [u.strip() for u in USERS_STR.split(',') if u.strip()]
WEBHOOK_URL = os.environ.get('DINGTALK_WEBHOOK')

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LAST_ID_FILE = os.path.join(BASE_DIR, 'last_id.json')

# è¿è¡Œæ¨¡å¼é…ç½®
LOOP_MODE = os.environ.get('LOOP_MODE', 'false').lower() == 'true'
INTERVAL = int(os.environ.get('LOOP_INTERVAL', '600')) # é»˜è®¤ 10 åˆ†é’Ÿ (600ç§’)

# å¤‡é€‰ Nitter å®ä¾‹ (ä»…ä½œä¸ºåŸŸåå‚è€ƒ)
NITTER_INSTANCES = [
    'https://xcancel.com',
    'https://nitter.privacyredirect.com',
    'https://nitter.poast.org',
    'https://nitter.hu',
    'https://nitter.moomoo.me',
    'https://nitter.net',
]

INSTANCES_FILE = os.path.join(BASE_DIR, 'instances.json')

def get_random_user_agent():
    ua_list = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/121.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0"
    ]
    return random.choice(ua_list)

def load_instances():
    """
    ä»æœ¬åœ°ç¼“å­˜åŠ è½½å¥åº·çš„ Nitter å®ä¾‹
    """
    if os.path.exists(INSTANCES_FILE):
        try:
            with open(INSTANCES_FILE, 'r', encoding='utf-8') as f:
                instances = json.load(f)
                if instances and isinstance(instances, list):
                    print(f"[ç³»ç»Ÿ] æˆåŠŸä»æœ¬åœ°ç¼“å­˜åŠ è½½ {len(instances)} ä¸ªå®ä¾‹")
                    return instances
        except Exception as e:
            print(f"[ç³»ç»Ÿ] åŠ è½½å®ä¾‹ç¼“å­˜å¤±è´¥: {e}")
    
    print("[ç³»ç»Ÿ] ç¼“å­˜ä¸å­˜åœ¨æˆ–æŸåï¼Œé‡‡ç”¨å†…ç½®å…œåº•å®ä¾‹åˆ—è¡¨")
    return NITTER_INSTANCES

def scrape_nitter_with_playwright(target, dynamic_instances=None):
    """
    ä½¿ç”¨ Playwright æ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—® Nitter å¹¶æŠ“å–æœ€æ–°æ¨æ–‡
    """
    is_search = target.startswith('search:')
    keyword = target[7:] if is_search else target
    
    # ä¼˜å…ˆä½¿ç”¨åŠ¨æ€è·å–çš„å®ä¾‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ç”¨å†…ç½®çš„
    instances = dynamic_instances if dynamic_instances else NITTER_INSTANCES.copy()
    # ä¸ºäº†åˆ†å¸ƒå‹åŠ›ï¼Œæˆ‘ä»¬åœ¨ä¿æŒé«˜åˆ†å®ä¾‹åœ¨å‰çš„å‰æä¸‹ï¼Œå¯¹å‰ 5 åè¿›è¡Œå°èŒƒå›´éšæœº
    if len(instances) > 5:
        top_5 = instances[:5]
        random.shuffle(top_5)
        others = instances[5:]
        random.shuffle(others)
        instances = top_5 + others
    else:
        random.shuffle(instances)
    
    with sync_playwright() as p:
        # å¯åŠ¨æµè§ˆå™¨ (å¤´æ¨¡å¼/æ— å¤´æ¨¡å¼å–å†³äºç¯å¢ƒï¼ŒGitHub Actions å»ºè®® headless=True)
        browser = p.chromium.launch(headless=True)
        
        for instance in instances:
            try:
                # æ¯ä¸ªå®ä¾‹åˆ›å»ºä¸€ä¸ªæ–°ä¸Šä¸‹æ–‡ï¼Œæ¨¡æ‹Ÿå¹²å‡€çš„è®¿é—®
                context = browser.new_context(
                    user_agent=get_random_user_agent(),
                    viewport={'width': 1280, 'height': 720}
                )
                page = context.new_page()
                
                # åº”ç”¨ Stealth æ’ä»¶ç»•è¿‡æ£€æµ‹
                stealth_sync(page)
                
                if is_search:
                    url = f"{instance.rstrip('/')}/search?f=tweets&q={requests.utils.quote(keyword)}"
                else:
                    url = f"{instance.rstrip('/')}/{keyword}"
                
                print(f"[{target}] æ­£åœ¨åŠ è½½: {url}")
                
                # å¼€å§‹åŠ è½½å¹¶å¤„ç†å¯èƒ½çš„æŒ‘æˆ˜
                try:
                    response = page.goto(url, wait_until="networkidle", timeout=45000)
                    if response and response.status == 403:
                        print(f"[{target}] è®¿é—® {instance} è¢«æ‹’ (403 Forbidden)")
                        context.close()
                        continue
                except Exception as e:
                    print(f"[{target}] åŠ è½½ {instance} è¶…æ—¶æˆ–å¤±è´¥: {e}")
                    context.close()
                    continue
                
                # æ™ºèƒ½ç­‰å¾…æµè§ˆå™¨éªŒè¯æˆ–"ç¨ç­‰ç‰‡åˆ»"æŒ‘æˆ˜
                challenge_keywords = ["Verifying your browser", "Just a moment", "Checking your browser"]
                for i in range(5): # æœ€å¤šç­‰å¾… 25 ç§’
                    content = page.content()
                    if any(kw in content for kw in challenge_keywords):
                        print(f"[{target}] æ£€æµ‹åˆ°æµè§ˆå™¨éªŒè¯ ({i+1}/5)ï¼Œå°è¯•ç­‰å¾…...")
                        page.wait_for_timeout(5000)
                    else:
                        break
                
                # è·å–æœ€ç»ˆæ¸²æŸ“åçš„ HTML
                soup = BeautifulSoup(page.content(), 'html.parser')
                
                # Nitter é¡µé¢æ¨æ–‡è§£æé€»è¾‘
                items = soup.select('.timeline-item')
                if not items:
                    print(f"[{target}] åœ¨å®ä¾‹ {instance} ä¸Šæœªå‘ç°æ¨æ–‡å†…å®¹")
                    context.close()
                    continue
                
                # æ‰«æç­–ç•¥ï¼šæ‰«æå‰ 8 æ¡æ¨æ–‡ï¼Œæ‰¾åˆ°ç¬¬ä¸€æ¡éç½®é¡¶çš„ã€æœ‰æ•ˆçš„å†…å®¹
                valid_tweets = []
                for item in items[:8]:
                    # 1. æ£€æŸ¥æ˜¯å¦æ˜¯ç½®é¡¶æ¨æ–‡ (ç§»é™¤ "Pinned" text åŒ¹é…ä»¥é˜²æ­¢è¯¯ä¼¤æ¨æ–‡å†…å®¹)
                    is_pinned = item.select_one('.pinned') is not None
                    if is_pinned:
                        print(f"[{target}] å‘ç°ç½®é¡¶æ¨æ–‡ï¼Œè·³è¿‡")
                        continue
                    
                    # 2. æ£€æŸ¥æ˜¯å¦æ˜¯è½¬å‘
                    is_retweet = item.select_one('.retweet-header') is not None

                    # 3. æå–å›¾ç‰‡ (å¢åŠ æ›´å¤šå¯èƒ½çš„ Nitter å›¾ç‰‡é€‰æ‹©å™¨)
                    images = []
                    img_els = item.select('.attachment.image img, .tweet-image img, .still-image img, .attachments img')
                    for img in img_els:
                        # æ’é™¤å¤´åƒ (é€šå¸¸åœ¨ .tweet-avatar æˆ– .profile-card-avatar ä¸­)
                        if any(c in str(img.parent.get('class', [])) for c in ['avatar', 'profile']):
                            continue
                            
                        src = img.get('src', '')
                        if src:
                            # è½¬æ¢ç›¸å¯¹è·¯å¾„
                            if src.startswith('//'):
                                full_src = 'https:' + src
                            elif src.startswith('/'):
                                full_src = instance.rstrip('/') + src
                            else:
                                full_src = src
                            
                            # è¿˜åŸåŸå§‹ Twitter å›¾ç‰‡é“¾æ¥ä»¥æé«˜ä»£ç†ç¨³å®šæ€§
                            full_src = get_original_image_url(full_src)
                            
                            # è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾çš„è¡¨æƒ…åŒ…æˆ–å°å›¾æ ‡ (å¯é€‰)
                            if 'emoji' in src.lower() or 'hashtag_click' in src:
                                continue
                                
                            images.append(full_src)

                    # 4. æå–è§†é¢‘ (æ–°å¢)
                    video_url = None
                    try:
                        video_el = item.select_one('video source')
                        if not video_el:
                            video_el = item.select_one('video')
                        
                        if video_el:
                            # å°è¯•è·å–å°é¢å›¾ä½œä¸ºé¢å¤–å›¾ç‰‡
                            poster_el = item.select_one('video')
                            if poster_el:
                                poster = poster_el.get('poster', '')
                                if poster:
                                    if poster.startswith('//'):
                                        full_poster = 'https:' + poster
                                    elif poster.startswith('/'):
                                        full_poster = instance.rstrip('/') + poster
                                    else:
                                        full_poster = poster
                                    # å°è¯•è¿˜åŸåŸå§‹å°é¢å›¾åœ°å€å¹¶åŠ å…¥å›¾ç‰‡åˆ—è¡¨
                                    full_poster = get_original_image_url(full_poster)
                                    if full_poster not in images:
                                        images.append(full_poster)

                            # æå–è§†é¢‘æµåœ°å€
                            v_src = video_el.get('src', '')
                            if v_src:
                                if v_src.startswith('//'):
                                    video_url = 'https:' + v_src
                                elif v_src.startswith('/'):
                                    video_url = instance.rstrip('/') + v_src
                                else:
                                    video_url = v_src
                    except Exception as e:
                        print(f"[{target}] è§†é¢‘æå–å¼‚å¸¸: {e}")

                    # æå–å…³é”®ä¿¡æ¯
                    content_el = item.select_one('.tweet-content')
                    link_el = item.select_one('.tweet-link')
                    date_el = item.select_one('.tweet-date a')
                    author_el = item.select_one('.username')

                    if not content_el or not link_el:
                        continue

                    # æå–æ¨æ–‡ ID (ä» /user/status/123...#m ä¸­æå–æ•°å­—)
                    link_href = link_el.get('href', '')
                    tweet_id = link_href.split('/status/')[-1].split('#')[0] if '/status/' in link_href else link_href

                    tweet_data = {
                        'content': content_el.get_text(strip=True),
                        'link': instance.rstrip('/') + link_href,
                        'published': date_el.get('title', '') if date_el else 'Unknown Time',
                        'author': author_el.get_text(strip=True) if author_el else keyword,
                        'guid': tweet_id,
                        'is_retweet': is_retweet,
                        'images': images,
                        'video_url': video_url
                    }
                    valid_tweets.append(tweet_data)
                    
                    # åªè¦æ‰¾åˆ°äº†ç¬¬ä¸€ä¸ªéç½®é¡¶çš„æœ‰æ•ˆæ¨æ–‡ï¼Œæˆ‘ä»¬å°±è®¤ä¸ºå®ƒæ˜¯å½“å‰â€œæœ€æ–°çš„â€
                    if len(valid_tweets) >= 1:
                        break

                if valid_tweets:
                    tweet = valid_tweets[0]
                    retweet_tag = " [è½¬å‘]" if tweet['is_retweet'] else ""
                    print(f"[{target}] æˆåŠŸä» {instance} æŠ“å–{retweet_tag}æ¨æ–‡: {tweet['guid']}")
                    context.close()
                    browser.close()
                    return tweet

                print(f"[{target}] {instance} é¡µé¢ä¸Šæœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„éç½®é¡¶æ¨æ–‡")
                context.close()

            except Exception as e:
                print(f"[{target}] è®¿é—® {instance} å‡ºé”™: {e}")
                continue
        
        browser.close()
    return None

def upload_to_imgbb(image_url):
    """
    ä¸Šä¼ å›¾ç‰‡åˆ° ImgBB å›¾åºŠ
    éœ€è¦é…ç½®ç¯å¢ƒå˜é‡: IMGBB_API_KEY
    """
    api_key = os.environ.get('IMGBB_API_KEY', '').strip()
    if not api_key:
        print("[å›¾åºŠ] ImgBB æœªé…ç½® API Key, æ— æ³•ä¸Šä¼ ")
        return None
    
    try:
        # ä¸‹è½½å›¾ç‰‡
        print(f"[å›¾åºŠ] æ­£åœ¨ä» {image_url} ä¸‹è½½å›¾ç‰‡...")
        img_response = requests.get(image_url, timeout=30, headers={
            'User-Agent': get_random_user_agent(),
            'Referer': 'https://twitter.com/'
        })
        img_response.raise_for_status()
        
        # è½¬æ¢ä¸º base64
        img_base64 = base64.b64encode(img_response.content).decode('utf-8')
        
        # ä¸Šä¼ åˆ° ImgBB
        print("[å›¾åºŠ] æ­£åœ¨ä¸Šä¼ åˆ° ImgBB...")
        upload_response = requests.post(
            'https://api.imgbb.com/1/upload',
            data={
                'key': api_key,
                'image': img_base64
            },
            timeout=30
        )
        result = upload_response.json()
        
        if result.get('success'):
            url = result['data']['url']
            print(f"[å›¾åºŠ] ImgBB ä¸Šä¼ æˆåŠŸ: {url}")
            return url
        else:
            print(f"[å›¾åºŠ] ImgBB ä¸Šä¼ å¤±è´¥: {result}")
            return None
    except Exception as e:
        print(f"[å›¾åºŠ] ImgBB ä¸Šä¼ å¼‚å¸¸: {e}")
        return None

def upload_image_to_bed(image_url):
    """
    ä¸Šä¼ å›¾ç‰‡åˆ° ImgBB å›¾åºŠ
    """
    return upload_to_imgbb(image_url)



def send_dingtalk(webhook_url, tweet, target):
    """
    å‘é€é’‰é’‰æ¶ˆæ¯
    """
    if not webhook_url:
        print("æœªé…ç½® DINGTALK_WEBHOOKï¼Œè·³è¿‡å‘é€")
        return False

    retweet_flag = " ğŸ”ƒ è½¬å‘äº†" if tweet.get('is_retweet') else " ğŸ“ å‘å¸ƒäº†"
    
    # å°è¯•ç¿»è¯‘å†…å®¹
    print(f"[{target}] æ­£åœ¨ç¿»è¯‘æ¨æ–‡å†…å®¹...")
    
    # æ¸…ç†åŸæ–‡ä¸­çš„ä¹±ç æˆ–è£…é¥°æ€§å­—ç¬¦
    raw_content = tweet['content']
    # ç§»é™¤ç‰¹å®šä¹±ç åºåˆ— â‚¬âˆ‹
    clean_content = raw_content.replace('â‚¬âˆ‹', '').strip()
    
    translated_content = translate_text(clean_content)
    
    # æ„é€ å†…å®¹å±•ç¤º (å¦‚æœæœ‰ç¿»è¯‘åˆ™æ˜¾ç¤ºç¿»è¯‘+åŸæ–‡)
    if translated_content:
        display_content = f"""**ç¿»è¯‘**: {translated_content}\n\n**åŸæ–‡**: {raw_content}"""
    else:
        display_content = f"""{raw_content}"""

    # æ„é€ å›¾ç‰‡ Markdown (ä¼˜å…ˆä½¿ç”¨å›¾åºŠ,å›é€€åˆ°ä»£ç†)
    images_md = ""
    if tweet.get('images'):
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨å›¾åºŠä¸Šä¼ 
        use_image_bed = os.environ.get('USE_IMAGE_BED', 'true').lower() == 'true'
        
        for img_url in tweet['images']:
            import urllib.parse
            
            final_url = None
            
            # æ–¹æ¡ˆ1: å°è¯•ä¸Šä¼ åˆ°å›¾åºŠ (æ¨è)
            if use_image_bed:
                print(f"[{target}] æ­£åœ¨ä¸Šä¼ å›¾ç‰‡åˆ°å›¾åºŠ...")
                final_url = upload_image_to_bed(img_url)
            
            # æ–¹æ¡ˆ2: å¦‚æœå›¾åºŠå¤±è´¥,ä½¿ç”¨ä»£ç†æœåŠ¡
            if not final_url:
                cloudflare_proxy = os.environ.get('CLOUDFLARE_PROXY', '').strip()
                if cloudflare_proxy:
                    encoded_url = urllib.parse.quote(img_url)
                    final_url = f"{cloudflare_proxy.rstrip('/')}?url={encoded_url}"
                else:
                    # å›é€€åˆ° wsrv.nl ä»£ç†
                    clean_url = img_url.replace('https://', '').replace('http://', '')
                    encoded_url = urllib.parse.quote(clean_url)
                    final_url = f"https://wsrv.nl/?url={encoded_url}"
            
            if final_url:
                images_md += f"\n\n![image]({final_url})"

    # å¦‚æœæœ‰è§†é¢‘é“¾æ¥ï¼Œæ·»åŠ è§‚çœ‹é“¾æ¥
    if tweet.get('video_url'):
        images_md += f"\n\n[ğŸ¬ ç‚¹å‡»è§‚çœ‹è§†é¢‘]({tweet['video_url']})"

    title = f"Twitter ç›‘æ§: {target}"
    text = f"""## {target}{retweet_flag} æ¨æ–‡
---
**ä½œè€…**: {tweet['author']}
**æ—¶é—´**: {tweet['published']}

> {display_content}
{images_md}

---
[ğŸ”— Nitter åŸæ–‡]({tweet['link']}) | [ğŸ”— Twitter(X) åŸæ–‡]({tweet['link'].replace('xcancel.com', 'twitter.com').replace('nitter.net', 'twitter.com').replace('nitter.hu', 'twitter.com').replace('nitter.privacyredirect.com', 'twitter.com').replace('nitter.poast.org', 'twitter.com')})
    """

    data = {
        "msgtype": "markdown",
        "markdown": {
            "title": title,
            "text": text
        }
    }

    try:
        resp = requests.post(webhook_url, json=data, timeout=10)
        result = resp.json()
        if result.get('errcode') == 0:
            print(f"[{target}] é’‰é’‰æ¨é€æˆåŠŸ")
            return True
        else:
            print(f"[{target}] é’‰é’‰æ¨é€å¤±è´¥: {result}")
            return False
    except Exception as e:
        print(f"[{target}] é’‰é’‰è¯·æ±‚å¼‚å¸¸: {e}")
        return False

def get_original_image_url(nitter_url):
    """
    å°è¯•ä» Nitter çš„ä»£ç† URL ä¸­è¿˜åŸå‡º Twitter/X çš„åŸå§‹å›¾ç‰‡åœ°å€
    ä¾‹å¦‚: /pic/media%2FGDR-yXfbsAA_JmS.jpg -> pbs.twimg.com
    """
    import urllib.parse
    import re
    try:
        if 'pbs.twimg.com' in nitter_url:
            return nitter_url
            
        # 1. å¤„ç† hex ç¼–ç çš„å¯¹è±¡ (å¸¸è§äº xcancel ç­‰å®ä¾‹)
        if '/pic/enc/' in nitter_url:
            enc_part = nitter_url.split('/pic/enc/')[-1].split('?')[0]
            try:
                decoded = bytes.fromhex(enc_part).decode('utf-8')
                if 'pbs.twimg.com' in decoded:
                    return decoded
            except:
                pass

        # 2. å¤„ç†æ ‡å‡† Nitter è·¯å¾„
        path = urllib.parse.unquote(nitter_url)
        
        # åŒ¹é… /pic/media/ID.ext æˆ– /pic/orig/media/ID.ext
        if '/media/' in path:
            media_part = path.split('/media/')[-1].split('?')[0]
            if '.' in media_part:
                media_id, ext = media_part.rsplit('.', 1)
                # æŸäº›æ—¶å€™ ext åé¢å¯èƒ½è¿˜è·Ÿç€ &name=...
                ext = ext.split('&')[0].split('?')[0]
                return f"https://pbs.twimg.com/media/{media_id}?format={ext}&name=large"

        # 3. å¤„ç†ç›´æ¥åŒ…å« pbs.twimg.com çš„è·¯å¾„ (å¦‚ /pic/pbs.twimg.com/media/...)
        if 'pbs.twimg.com' in path:
            # æå–ä» pbs.twimg.com å¼€å§‹çš„éƒ¨åˆ†
            match = re.search(r'(pbs\.twimg\.com/media/[^?&]+)', path)
            if match:
                return "https://" + match.group(1)

    except Exception as e:
        print(f"[å›¾ç‰‡è§£æ] è¿˜åŸ URL å¤±è´¥ {nitter_url}: {e}")
        
    return nitter_url

def translate_text(text, target_lang='zh-CN'):
    """
    ä½¿ç”¨ Google Translate (GTX) æ¥å£è¿›è¡Œå…è´¹ç¿»è¯‘
    """
    if not text or not text.strip():
        return ""
    
    # ç®€å•çš„ç¿»è¯‘é€»è¾‘
    try:
        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            "client": "gtx",
            "sl": "auto",
            "tl": target_lang,
            "dt": "t",
            "q": text
        }
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        resp = requests.get(url, params=params, headers=headers, timeout=15)
        resp.raise_for_status()
        
        # è§£æè¿”å›çš„ JSON
        data = resp.json()
        if data and data[0]:
            translated_parts = [part[0] for part in data[0] if part[0]]
            return "".join(translated_parts)
    except Exception as e:
        print(f"[ç¿»è¯‘] å¤±è´¥: {e}")
    
    return None

def main():
    if not USERS:
        print("æ²¡æœ‰é…ç½®ç›‘æ§ç›®æ ‡")
        return

    print(f"[{datetime.now()}] å¯åŠ¨ç›‘æ§æ¨¡å¼ (LOOP_MODE={LOOP_MODE}, INTERVAL={INTERVAL}s)...")
    
    # ä»æœ¬åœ°ç¼“å­˜åŠ è½½å¯ç”¨å®ä¾‹
    instances = load_instances()

    while True:
        cycle_start = time.time()
        print(f"\n--- å¯åŠ¨æ–°ä¸€è½®ç›‘æ§è½®è¯¢ [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ---")
        
        # åŠ è½½çŠ¶æ€ (æ¯è½®éƒ½é‡æ–°åŠ è½½ï¼Œé˜²æ­¢å¤–éƒ¨æ‰‹åŠ¨ä¿®æ”¹æˆ–å¼‚å¸¸)
        if os.path.exists(LAST_ID_FILE):
            try:
                with open(LAST_ID_FILE, 'r', encoding='utf-8') as f:
                    last_ids = json.load(f)
            except: last_ids = {}
        else: last_ids = {}

        updated = False
        for target in USERS:
            try:
                tweet = scrape_nitter_with_playwright(target, instances)
                if not tweet:
                    continue
                
                current_id = tweet['guid']
                if last_ids.get(target) != current_id:
                    print(f"[{target}] å‘ç°æ›´æ–°: {current_id}")
                    if send_dingtalk(WEBHOOK_URL, tweet, target):
                        last_ids[target] = current_id
                        updated = True
                else:
                    print(f"[{target}] æ— è§†æ›´æ–° (ID æœªå˜)")
            except Exception as e:
                print(f"[{target}] å¤„ç†å¼‚å¸¸: {e}")

        if updated:
            with open(LAST_ID_FILE, 'w', encoding='utf-8') as f:
                json.dump(last_ids, f, indent=2, ensure_ascii=False)
            print("[ç³»ç»Ÿ] çŠ¶æ€æ–‡ä»¶å·²æ›´æ–°")

        if not LOOP_MODE:
            print("[ç³»ç»Ÿ] éå¾ªç¯æ¨¡å¼ï¼Œä»»åŠ¡ç»“æŸã€‚")
            break
        
        # è®¡ç®—éœ€è¦ sleep çš„æ—¶é—´ï¼Œå‡å»å·²ç»æ¶ˆè€—çš„æ—¶é—´
        elapsed = time.time() - cycle_start
        sleep_time = max(10, INTERVAL - elapsed)
        print(f"--- è½®è¯¢ç»“æŸã€‚è€—æ—¶ {elapsed:.1f}sï¼Œå‡†å¤‡ä¼‘çœ  {sleep_time:.1f}s ---\n")
        time.sleep(sleep_time)

if __name__ == "__main__":
    main()
